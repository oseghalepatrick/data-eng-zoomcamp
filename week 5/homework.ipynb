{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41800305",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "\n",
    "Execute `spark.version`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac27ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/oseghalepatrick53/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/27 11:09:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38362044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cfd7f",
   "metadata": {},
   "source": [
    "## Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec6e0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-26 20:52:00--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.172.169\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.172.169|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘data/raw/fhvhv/fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  33.7MB/s    in 25s     \n",
      "\n",
      "2022-02-26 20:52:25 (28.2 MB/s) - ‘data/raw/fhvhv/fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P data/raw/fhvhv/ https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c7f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/raw/fhvhv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582f98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 11 data/raw/fhvhv/fhvhv_tripdata_2021-02.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8938b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e613b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7941ec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965da81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c735750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090c8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('data/raw/fhvhv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9d8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 700M\r\n",
      "-rw-rw-r-- 1 oseghalepatrick53 oseghalepatrick53 700M Oct 29 18:53 fhvhv_tripdata_2021-02.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/raw/fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b493796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .repartition(24)\\\n",
    "    .write.parquet('data/pq/fhvhv/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af082a6d",
   "metadata": {},
   "source": [
    "`What's the size of the folder with results (in MB)?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c700d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 208M\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53    0 Feb 27 11:14 _SUCCESS\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00000-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00001-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00002-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00003-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00004-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00005-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00006-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:13 part-00007-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00008-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00009-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00010-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00011-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00012-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00013-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00014-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00015-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00016-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00017-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00018-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00019-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00020-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00021-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00022-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 oseghalepatrick53 oseghalepatrick53 8.7M Feb 27 11:14 part-00023-94f70641-8e2a-444f-bfd7-35e21cc82b4b-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhvhv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b40c69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet('data/pq/fhvhv/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca1f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b0831",
   "metadata": {},
   "source": [
    "## Question 3. Count records\n",
    "`How many taxi trips were there on February 15?`\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92a0c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|trips|\n",
      "+-----+\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:==============>                                            (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(1) AS trips\n",
    "FROM \n",
    "    fhvhv\n",
    "WHERE \n",
    "    pickup_datetime = '2021-02-15 00:00:00'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e09b63",
   "metadata": {},
   "source": [
    "## Question 4. Longest trip for each day\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "`Trip starting on which day was the longest?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e414e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:========================================>              (147 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      Date|max_seconds|\n",
      "+----------+-----------+\n",
      "|2021-02-11|      75540|\n",
      "|2021-02-17|      57221|\n",
      "|2021-02-20|      44039|\n",
      "|2021-02-03|      40653|\n",
      "|2021-02-19|      37577|\n",
      "|2021-02-25|      35010|\n",
      "|2021-02-18|      34612|\n",
      "|2021-02-10|      34169|\n",
      "|2021-02-21|      32223|\n",
      "|2021-02-09|      32087|\n",
      "|2021-02-06|      31447|\n",
      "|2021-02-02|      30913|\n",
      "|2021-02-05|      30511|\n",
      "|2021-02-12|      30148|\n",
      "|2021-02-08|      30106|\n",
      "|2021-02-14|      29777|\n",
      "|2021-02-22|      28278|\n",
      "|2021-02-27|      27170|\n",
      "|2021-02-15|      25874|\n",
      "|2021-02-04|      25592|\n",
      "|2021-02-16|      25441|\n",
      "|2021-02-23|      24439|\n",
      "|2021-02-26|      24422|\n",
      "|2021-02-24|      23669|\n",
      "|2021-02-13|      21442|\n",
      "|2021-02-01|      20638|\n",
      "|2021-02-28|      19850|\n",
      "|2021-02-07|      17672|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:=====================================================> (193 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_format(pickup_datetime, \"yyyy-MM-dd\") AS Date,\n",
    "    max(unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) as max_seconds\n",
    "FROM \n",
    "    fhvhv\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "\"\"\").show(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbd47",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` in this dataset.\n",
    "\n",
    "How many stages this spark job has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ff55091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>        (170 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|dispatching_base_num|\n",
      "+--------------------+\n",
      "|              B02510|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dispatching_base_num\n",
    "FROM (\n",
    "    SELECT \n",
    "        dispatching_base_num,\n",
    "        count(1)\n",
    "    FROM \n",
    "        fhvhv\n",
    "    GROUP BY\n",
    "        1\n",
    "    ORDER BY\n",
    "        2 DESC\n",
    "    LIMIT \n",
    "        1\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aeb67b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b8d1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c153def7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 146:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|pickup_dropoff_zone        |\n",
      "+---------------------------+\n",
      "|East New York/East New York|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pickup_dropoff_zone\n",
    "FROM (\n",
    "    SELECT \n",
    "        CONCAT(COALESCE(puzone.zone, \"Unknown\"),'/', COALESCE(dozone.zone, \"Unknown\")) AS pickup_dropoff_zone,\n",
    "        COUNT(1) AS trips\n",
    "    FROM \n",
    "        fhvhv\n",
    "    LEFT JOIN \n",
    "        zones AS puzone\n",
    "        ON fhvhv.PULocationID = puzone.LocationID\n",
    "    LEFT JOIN\n",
    "        zones AS dozone\n",
    "        ON fhvhv.DOLocationID = dozone.LocationID\n",
    "    GROUP BY\n",
    "        1\n",
    "    ORDER BY \n",
    "        2 DESC\n",
    "    LIMIT 1\n",
    ")\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bfc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
